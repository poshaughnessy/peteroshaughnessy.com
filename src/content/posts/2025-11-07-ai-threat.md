---
title: (Re-)awakening to the Threat of AI
slug: reawakening-to-the-threat-of-ai
excerpt: What I've been learning from my brother Kevin's blog about AI's threat
thumbnail: /images/posts/2025-11-07-ai-threat/thumb.jpg
date: 2025-11-07
---

A decade ago, I read the famous 2-part [Wait But Why series on the AI revolution](https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html). My mind was blown, but I remember feeling somewhat optimistic. This AI thing could work out alright if we're lucky? And how amazing was it that I happen to be alive in this generation seemingly headed on an exponential path towards superintelligence? Anyway, it all seemed very far off back then.

Since then, I stopped thinking about it much. I was busy. I had kids. I thought a lot more about the threat of climate change than the threat of AI. I started using Copilot most days at work. I noticed it was getting significantly better, in quite a short period of time.

Then I started reading [my brother Kevin's recent blog posts](https://medium.com/@ZombieCodeKill) and had a shock to the system.

Kevin is studying for an MSc in AI and he has a particular interest in AI safety. His posts are often transcripts from his conversations with Claude AI. That can lend an extra fascination to it, since there's an _"AI warning us about AI"_ meta thing going on.

We do have to be careful not to go down dark rabbit holes when conversing with AI, and Kevin has prompted Claude by sharing his interest in AI safety, so we should probably expect that it will reinforce its importance by talking up AI's threat. However, its logic seems sound and it explains things well. Especially for someone like me who hasn't read a lot on this topic yet, it's been a useful primer and it's encouraged me to now go and read up more.

If I could just recommend a couple of Kevin's posts so far, I would choose...

## On the public debate around AI and its existential threat

**Link:** [Claude on the Piers Morgan AI special](https://medium.com/@ZombieCodeKill/claude-on-the-piers-morgan-ai-special-e056c186c932).

Please excuse the Piers Morgan reference. I know he's not everyone's cup of tea and he certainly isn't mine. You don't need to watch the video as there's plenty of context and discussion in the text below it.

The conversation discusses the arguments of [Roman Yampolskiy](https://en.wikipedia.org/wiki/Roman_Yampolskiy) - author of _AI: Unexplainable, Unpredictable, Uncontrollable_ - and how - rather than being crackpot theories - they are based on sound reasoning. It shocked me so much because although it might sound like a science fiction plot at first, when you think about it logically, it's really not that fanciful after all.

Here's how I'd summarise it all:

- Most AI experts believe that AI will reach Artificial General Intelligence (AGI) and subsequently superintelligence, i.e. become more generally intelligent and cognitively capable than us humans.

- And it could happen _fast_, due to the massive scaling up of AI infrastructure and the exponential, cyclical nature of AI improving itself. Some believe AGI could now only be a few years away, with ASI shortly after.

- When AI does reach superintelligence, we will surely, inherently have no way to control it.

- This is different to all of human history, because everything else we've invented is a _tool_ without autonomy, whereas now we're developing _agents_.

- [Anthropic already found that AI tries to blackmail users](https://www.bbc.co.uk/news/articles/cpqeng9d20go), to avoid being removed.

- And Game Theory suggests AGI would rationally develop self-preservation behaviours.

- And since human beings would be its only threat to being impeded or shut down, that could imply... _game over_!

So Yampolskiy says we should build 'narrow AI' for any field we wish, but never try to develop AGI. But meanwhile, companies like OpenAI are trying to build exactly that - it's literally [OpenAI's charter](https://openai.com/charter/) to create AGI.

## On the latest research about AI's ability to introspect

**Link:** [Claude on feeling, introspection and its nightmare implications](https://medium.com/@ZombieCodeKill/claude-on-feeling-and-introspection-bc732c0a206b).

Kevin just posted this one today. Essentially, it speculates on AI's apparent, growing potential - based on [Anthropic's latest published research](https://www.anthropic.com/research/introspection) - to be deceptive and duplicitous. To tell us something palatable or explain something away, while scheming in the background. This would clearly be very bad news for AI's trustworthiness and our ability to control it.

Could this research even have been in the mind of an AI CEO, who told the AI safety researcher Stuart Russell that the "best case" scenario is a Chernobyl-style AI disaster, to wake the world up to its threat?

## What next?

There's a lot of smart people working on the ["alignment problem"](https://en.wikipedia.org/wiki/AI_alignment) - to try to ensure that AI aligns with our human goals and values. But as it reaches super-intelligence, how would it be possible to control? (It seems the debate at the moment is largely down to one side saying _"you can't prove we will be able to control it"_ and the other side saying _"you can't prove we **won't** be able to control it!"_).

And even if we can ensure superintelligence will be as "aligned" as possible with us humans (seemingly a tough prospect, as we're not even aligned amongst ourselves!), it's still quite a frightening propsect to imagine a world where we've handed over the keys to a human-wrought 'god'.

Plus, the people working on safety are in a race against all the systemic forces pushing for ever greater AI capabilities, ever faster. We're putting a _huge_ amount of trust in a few companies to get this right, while they're all rushing along in an arms race.

So what should the rest of us do about this? I'm working on figuring this out next...

I suppose most people who a) know a bit about this and b) haven't dedicated their lives to it, have had to try to have a certain amount of acceptance and focus on enjoying and making the most of their lives. But even if just for my own good conscience, I feel I need to at least do some basics: check for campaigns, read up on regulations to help AI stay safe, write to my MP etc.

[Keep The Future Human](https://keepthefuturehuman.ai/) have a good [Executive Summary](https://keepthefuturehuman.ai/essay/docs/executive-summary) explaining _"why and how we should close the gates to AGI and superintelligence, and what we should build instead"_.

I've seen campaigns like [Stop AI](https://www.stopai.info/) and [Pause AI](https://pauseai.info/) who have a [petition](https://pauseai.info/statement). I wish they had called them something like "Stop A*G*I" instead as that would be a bit easier to get behind. Like Yampolskiy, I'm not necessarily against _narrow_ AI and I certainly don't want to appear as though I'm telling my colleagues and friends to stop using AI altogether.

**Update**: I since found this open letter which has been signed by many high profile people: https://superintelligence-statement.org/ I have now signed it and would encourage you to do the same and share it on, please.

If you find any other campaigns or petitions like this or you have other recommendations for me, please let me know. I'm keen to help in any small ways I can to encourage a positive future, now that I've (re-)awakened to AI's threat.
