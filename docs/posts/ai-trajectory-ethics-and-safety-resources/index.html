<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>AI trajectory, ethics and safety resources - Peter O'Shaughnessy</title>
    <meta name="description" content="Web developer especially interested in web APIs and the future of the web">
    <link rel="stylesheet" href="/styles/styles.css"/>
    <link rel="icon" href="/images/favicon.png" type="image/png">
    <link rel="alternate" type="application/rss+xml" title="RSS feed" href="https://peteroshaughnessy.com/rss.xml" />
    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@poshaughnessy">
    <meta name="twitter:creator" content="@poshaughnessy">
      <meta name="twitter:title" content="AI trajectory, ethics and safety resources">
      <meta name="twitter:description" content="A collection of links I found helpful for speed-running AI trajectory, ethics and safety topics">
      <meta name="twitter:image" content="https://peteroshaughnessy.com/images/posts/2025-12-31-ai-ethics-safety-resources/thumb.jpg">
  </head>
  <body>
    <header>
      <div>
        <div class="avatar-container">
          <a href="/"><img class="avatar" src="/images/peter-cartoon-circle.png" alt="Peter's cartoon avatar"/></a>
        </div>
        <div class="title-and-tagline-container">
          <h1><a href="/">Peter O'Shaughnessy</a></h1>
          <h2>Front-end developer now most concerned about AI safety</h2>
        </div>
      </div>
      <nav>
        <ul>
          <li><a href="/">Blog</a></li>
          <li><a href="/projects/">Projects</a></li>
          <li><a href="/talks/">Talks</a></li>
          <li><a href="/about/">About</a></li>
        </ul>
      </nav>
    </header>
<div class="contents">
  <article class="page page-post">
    <h1>AI trajectory, ethics and safety resources</h1>
    <p class="date"><time>24th Jun 2024</time></p>
    <p>After recently reading <a href="https://medium.com/@ZombieCodeKill">blog posts from my brother Kevin</a> who is studying for an MSc in AI, I became very concerned about the &quot;race to AGI&quot; (Artificial General Intelligence).</p>
<p>There really is no concensus on how close we are to AGI, nor even a consensus on the exact definition. However, using services from companies explicitly aiming to reach AGI to replace human labour (OpenAI, Microsoft, Google et al) feels to me rather like <a href="https://en.wikipedia.org/wiki/Turkeys_voting_for_Christmas">turkeys voting for Christmas</a>. And that&#39;s not even getting on to the existential risks.</p>
<p>So I have spent the past couple of months trying to catch up, reading and listening to lots and lots on AI: how it works, its capabilities and trajectory, near-term safety risks, ethical concerns, the risk of superintelligence and more. Particularly to help share with colleagues in our Ethics and Sustainability community, I have collected a lot of links. I thought I would share them here too, them to help anyone else wanting to &quot;speed run&quot; these topics. As <a href="www.ted.com/talks/tristan_harris_why_ai_is_our_ultimate_test_and_greatest_invitation">Tristan Harris says</a>, the first thing we can do to help right now is to try to increase public awareness and clarity.</p>
<p>Given the links are broadly about ethics and safety, there is naturally more of a focus on the dangers of AI. However, I have also been mindful to try to include some counter-arguments and differing views, to help paint a fuller picture. I will try to keep this list of resources up to date every now and then. Please do <em>not</em> view it as comprehensive, but more as a starting point. I hope you might find it useful.</p>
<h2>1. AI capability and trajectory</h2>
<p>Content which can help us to understand how AI is advancing and how near or far we might be from Artifcial General Inteligence (AGI). <em>Note:</em> Alternative terms for AGI include “powerful AI” (Dario Amodei’s preferred term) and “Higher Level Machine Intelligence” (HLMI, used by the ESPAI survey, for “when unaided machines can accomplish every task better and more cheaply than human workers”). This seems roughly equivalent to the term “strong AGI” used in the ‘Common Ground between AI 2027 &amp; AI as Normal Technology’ essay.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://time.com/6300942/ai-progress-charts/">TIME - 4 Charts That Show Why AI Progress Is Unlikely to Slow Down</a></li>
<li><a href="https://epoch.ai/blog/what-will-ai-look-like-in-2030">Epoch AI - What will AI look like in 2030?</a></li>
<li><a href="https://asteriskmag.substack.com/p/common-ground-between-ai-2027-and">Common Ground between AI 2027 &amp; AI as Normal Technology</a></li>
<li><a href="https://edition.cnn.com/2025/10/18/business/ai-bubble-analyst-nightcap">CNN - Why this analyst says the AI bubble is 17 times bigger than the dot-com bust</a></li>
<li><a href="https://www.theguardian.com/technology/2025/dec/19/data-centers-ai-investment">The Guardian - Investment in data centers worldwide hit record $61bn in 2025, report finds</a></li>
<li><a href="https://www.theguardian.com/commentisfree/2025/dec/23/artificial-intelligence-ai-bubble-bursts-humans-take-back-control">The Guardian - When the AI bubble bursts, humans will finally have their chance to take back control</a></li>
<li><a href="https://www.ft.com/content/728b03a4-cef3-4ee9-a421-d681998ef7d8">FT - AI upheaval shows little sign of lessening</a> (Dec 2025, paywalled)</li>
<li><a href="https://www.scientificamerican.com/article/how-close-are-todays-ai-models-to-agi-and-to-self-improving-into/">Scientific American - Are We Seeing the First Steps Toward AI Superintelligence?</a></li>
<li><a href="https://www.nbcnews.com/tech/innovation/andrew-ng-says-ai-limited-wont-replace-humans-anytime-soon-rcna246074">NBC - An AI pioneer [Andrew Ng] says the technology is &#39;limited&#39; and won&#39;t replace humans anytime soon</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://medium.com/@clairedigitalogy/llms-gans-and-diffusion-models-breaking-down-ai-jargon-80ffd54e83ff">LLMs, GANs, and Diffusion Models: Breaking Down AI Jargon</a></li>
<li><a href="https://thezvi.substack.com/p/yes-ai-continues-to-make-rapid-progress">Zvi Mowshowitz - Yes, AI Continues To Make Rapid Progress, Including Towards AGI</a> (Sep 2025)</li>
<li><a href="https://yosefk.com/blog/llms-arent-world-models.html">LLMs aren&#39;t world models</a></li>
<li><a href="https://medium.com/@sanderink.ursina/jepa-a-family-of-predictive-architectures-for-human-like-ai-ae9b55e92dc7">JEPA: A Family of Predictive Architectures for Human-Like AI</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai">2023 Expert Survey on Progress in AI (ESPAI)</a></li>
<li><a href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/">Metaculus - Date of Artificial General Intelligence</a></li>
<li><a href="https://theagiclock.com/">The AGI Clock</a> (aggregated expert predictions)</li>
<li><a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Metr - Measuring AI Ability to Complete Long Tasks</a> (exponential improvement at 50% reliability)</li>
<li><a href="https://arxiv.org/pdf/2510.18212">A Definition of AGI</a> (paper)</li>
<li><a href="https://research.aimultiple.com/ai-hallucination/">AI Hallucination: Compare top LLMs</a> (Dec 2025)</li>
<li><a href="https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models">LLM Hallucinations in 2025</a></li>
<li><a href="https://www.oreilly.com/radar/what-if-ai-in-2026-and-beyond/">O&#39;Reilly - What If? AI in 2026 and Beyond</a></li>
<li><a href="https://80000hours.org/agi/guide/when-will-agi-arrive/">The case for AGI by 2030</a></li>
<li><a href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">Frontier AI: capabilities and risks</a> - UK government research, Oct 2023</li>
<li><a href="https://www.anthropic.com/research/introspection">Anthropic - Signs of introspection in large language models</a></li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://paulkrugman.substack.com/p/talking-with-paul-kedrosky">Paul Krugman - Talking With Paul Kedrosky</a></li>
<li><a href="https://youtu.be/aR20FWCCjAs">Ilya Sutskever – We&#39;re moving from the age of scaling to the age of research</a></li>
<li><a href="https://youtu.be/HcyUUvxkykE">The AI Daily Brief: 51 Charts That Will Shape AI in 2026</a> (Dec 2025)</li>
<li><a href="https://www.dwarkesh.com/p/andrej-karpathy">Andrej Karpathy - AGI is still a decade away</a> (Oct 2025)</li>
</ul>
<h3>Quotes</h3>
<ul>
<li><em>&quot;Considering that our intelligence is fixed and machine intelligence is growing, it is only a matter of time before machines surpass us...&quot;</em><ul>
<li><a href="https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/#why-experts-believe-agi-is-inevitable-key-arguments-evidence">AIMultiple - Why experts believe AGI is inevitable</a></li>
</ul>
</li>
<li><em>&quot;What I know from the technical people I speak to here in Silicon Valley is the vast majority of them are very dubious that we are even on the path towards some kind of superintelligent, sentient being. The vast majority of them think that the current moment we&#39;re in with LLMs... doesn&#39;t even put us on the track to that sort of stuff...&quot;</em><ul>
<li>Jacob Ward, journalist and author (CNN, PBS, Al Jazeera, The Loop) on the <a href="https://theairisknetwork.substack.com/p/what-we-lose-when-ai-makes-choices">For Humanity podcast</a></li>
</ul>
</li>
</ul>
<h2>2. Current and near-term AI safety risks and ethics</h2>
<p>Content which can help us to understand near-term harms and risks from AI.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-chatgpt-1234804367/">Rolling Stone - These Women Tried To Warn Us About AI</a></li>
<li><a href="https://www.rollingstone.com/culture/culture-features/spiralist-cult-ai-chatbot-1235463175/">Rolling Stone - This Spiral-Obsessed AI ‘Cult’ Spreads Mystical Delusions Through Chatbots</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.anthropic.com/research/agentic-misalignment">Anthropic - Agentic misalignment</a></li>
<li><a href="https://www.gov.uk/government/collections/responsible-ai-toolkit">Gov.UK Responsible AI Toolkit</a></li>
<li><a href="https://futureoflife.org/ai-safety-index-winter-2025/">Future of Life Institute - AI Safety Index - Winter 2025</a></li>
<li><a href="https://www.dair-institute.org/">Distributed AI Research (DAIR) Institute</a> (founded by Timnit Gebru to “counter Big Tech’s pervasive influence on AI research, development and deployment”)</li>
<li><a href="https://partnershiponai.org/">Partnership on AI (PAI)</a> (non-profit for positive AI outcomes)</li>
<li><a href="https://www.aisi.gov.uk/">AI Security Institute</a> (UK government department)</li>
<li><a href="https://humane-intelligence.org/">Humane Intelligence</a> (non-profit for making AI “more accountable, responsible, and fair”)</li>
<li><a href="https://civai.org/">CivAI (Civic AI Security Program)</a> (non-profit that educates about AI&#39;s capabilities and dangers using live demonstrations)</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://youtu.be/yZtVuo07j7o">UK Deputy PM David Lammy&#39;s address to the UN Security Council on AI threat</a></li>
<li><a href="https://www.dair-institute.org/maiht3k/">Mystery AI Hype Theater 3000</a> (AI ethics and anti AI hype podcast)</li>
<li><a href="https://youtu.be/vPaF6TypfX8">BBC News - How hackers are using AI and how to protect yourself</a> (Nov 2025)</li>
</ul>
<h2>3. Superintelligence risks</h2>
<p>Content about the risk of artificial superintelligence (ASI).</p>
<h3>Petitions / Statements</h3>
<ul>
<li><a href="https://aistatement.com/">Statement on AI Risk</a> (please consider signing)</li>
<li><a href="https://superintelligence-statement.org/">Statement on Superintelligence</a> (please consider signing)</li>
<li><a href="https://controlai.com/statement">ControlAI Statement</a> (UK, please consider sending to your MP)</li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://www.bbc.co.uk/news/uk-65746524">BBC News - Artificial intelligence could lead to extinction, experts warn</a> (May 2025)</li>
<li><a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/02/jared-kaplan-artificial-intelligence-train-itself">The Guardian - ‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself</a> (Dec 2025)</li>
<li><a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/01/its-going-much-too-fast-the-inside-story-of-the-race-to-create-the-ultimate-ai">The Guardian - ‘It’s going much too fast’: the inside story of the race to create the ultimate AI</a> (Dec 2025)</li>
<li><a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/30/the-office-block-where-ai-doomers-gather-to-predict-the-apocalypse">The Guardian - The office block where AI ‘doomers’ gather to predict the apocalypse</a> (Dec 2025)</li>
<li>Counter-argument: <a href="https://www.noemamag.com/the-politics-of-superintelligence/">The Politics of Superintelligence</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.narrowpath.co/introduction">Narrow Path</a></li>
<li><a href="https://keepthefuturehuman.ai/">Keep The Future Human</a></li>
<li><a href="https://aisafety.info/">AISafety.info</a></li>
<li><a href="https://controlai.com/">ControlAI</a> (UK and global campaign)</li>
<li><a href="https://www.thecompendium.ai/">The Compendium</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">Wait But Why - The AI Revolution: The Road to Superintelligence</a> (2015)</li>
<li><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html">Wait But Why - The AI Revolution: Our Immortality or Extinction</a> (2015)</li>
<li><a href="https://blog.samaltman.com/machine-intelligence-part-1">Sam Altman - Machine Intelligence Part 1</a> (2015)</li>
<li><a href="https://blog.samaltman.com/machine-intelligence-part-2">Sam Altman - Machine Intelligence Part 2</a> (2015)</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.ted.com/talks/tristan_harris_why_ai_is_our_ultimate_test_and_greatest_invitation">TED Talk - Tristan Harris - Why AI is our ultimate test and greatest invitation</a> (written version <a href="https://centerforhumanetechnology.substack.com/p/the-narrow-path-why-ai-is-our-ultimate">here</a>)</li>
<li><a href="https://youtu.be/CYgcJoOwysY">AI Expert: We Have 2 Years Before Everything Changes | Tristan Harris | Diary Of A CEO</a> (please look past the scary Terminator style preview image) (podcast version <a href="https://episode.flightcast.com/01KB0GWEH63PB47R8VEVSN95KW.mp3">here</a>)</li>
<li><a href="https://your-undivided-attention.simplecast.com/episodes/feed-drop-into-the-machine-with-tobias-rose-stockwell-DghO_Lhb">Your Undivided Attention - Feed Drop: &quot;Into the Machine&quot; with Tobias Rose-Stockwell and Tristan Harris</a></li>
<li><a href="https://podcast.futureoflife.org/">Future of Life Institute Podcast</a></li>
<li><a href="https://theairisknetwork.substack.com/s/for-humanity-an-ai-risk-podcast">For Humanity: An AI Risk Podcast</a></li>
<li><a href="https://www.longviewinvestigations.com/">The Last Invention Podcast</a></li>
</ul>
<h3>Books</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">Nick Bostrom - Superintelligence</a> (2014)</li>
</ul>
<h3>Quotes</h3>
<ul>
<li><em>“Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.”</em><ul>
<li>Sam Altman in his <a href="https://blog.samaltman.com/machine-intelligence-part-1">2015 blog post</a>.</li>
</ul>
</li>
<li><em>“AI CEOs claim they know how to build superhuman AI, yet none can show how they&#39;ll prevent us from losing control – after which humanity&#39;s survival is no longer in our hands. I&#39;m looking for proof that they can reduce the annual risk of control loss to one in a hundred million, in line with nuclear reactor requirements. Instead, they admit the risk could be one in ten, one in five, even one in three, and they can neither justify nor improve those numbers.”</em><ul>
<li>Prof. Stuart Russell, Professor of Computer Science at UC Berkeley</li>
</ul>
</li>
<li><em>&quot;We&#39;re currently releasing the most powerful, inscrutable, uncontrollable technology we&#39;ve ever invented, that&#39;s already demonstrating behaviours of self-preservation and deception... We&#39;re releasing it faster than we&#39;ve released any other technology in history, and with the maximum incentive to cut corners on safety.&quot;</em><ul>
<li>Tristan Harris, Co-Founder, Center for Humane Tech</li>
</ul>
</li>
<li><em>“AI companies are racing to build technology that they absolutely do not understand and cannot control.”</em><ul>
<li>Steven Adler, former lead safety researcher at OpenAI</li>
</ul>
</li>
</ul>
<h2>4. Job replacement</h2>
<p>Content about the impact on jobs as AI advances and potentially reaches AGI.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic">Axios - Behind the Curtain: A white-collar bloodbath</a> (2025)</li>
<li><a href="https://www.theguardian.com/business/2025/dec/15/universal-basic-income-ai-andrew-yang">The Guardian - Why universal basic income still can’t meet the challenges of an AI economy</a></li>
<li><a href="https://restofworld.org/2025/engineering-graduates-ai-job-losses/">“Everyone is so panicked”: Entry-level tech workers describe the AI-fueled jobpocalypse</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://chrisbarber.co/I+asked+AI+researchers+and+economists+about+SWE+career+strategies+given+AI">I asked AI researchers and economists about SWE career strategies given AI</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">SignalFire State of Tech Talent Report 2025</a> (&quot;New grad hiring drops 50%...&quot;)</li>
<li><a href="https://www.ippr.org/media-office/up-to-8-million-uk-jobs-at-risk-from-ai-unless-government-acts-finds-ippr">Institute for Public Policy Research (IPPR)</a> (&quot;8 million UK jobs at risk&quot;)</li>
<li><a href="https://www.stlouisfed.org/on-the-economy/2025/aug/is-ai-contributing-unemployment-evidence-occupational-variation">Is AI Contributing to Rising Unemployment? Evidence from Occupational Variation</a> (“occupations with higher AI exposure experienced larger unemployment rate increases”)</li>
</ul>
<h3>Quotes</h3>
<ul>
<li>AGI will be <em>“like a flood of millions of new digital immigrants [of] Nobel Prize level capability [who] work at superhuman speed [and] will work for less than minimum wage.“</em><ul>
<li>Tristan Harris</li>
</ul>
</li>
<li><em>&quot;You know, we had the elevator man, and now we have automated elevators. We had bank tellers, now we have automated teller machines. So humans will always just find something else to do. But why is AI different than that? Because it’s intelligence, because it’s general intelligence. That means that rather than a technology that automates just bank tellers, this is automating all forms of human cognitive labor, meaning everything that a human mind can do. So who’s going to retrain faster? You moving to that other kind of cognitive labor or the AI that is trained on everything and can multiply itself by 100 million times and is retraining how to do that kind of labor?&quot;</em><ul>
<li>Tristan Harris</li>
</ul>
</li>
<li>Counter-argument: <em>&quot;I think we live in a very complex, high-friction world where actually intelligence is so multi-dimensional and all this &#39;replacement&#39; is just incredibly hard and in the real world when you&#39;re at the coal face... and humans are just so versatile and so well evolved to deal with the world that we actually live in... I think [AI] will work extraordinarily well in domains that are extraordinarily valuable, but I do not think we&#39;re heading for this frictionless, plug-and-play, is-a-human replacement, so I think humans will have comparative advantage in a bunch of things for a very long time.&quot;</em><ul>
<li>Matt Clifford, ex government AI advisor, on the Rest Is Politics podcast</li>
</ul>
</li>
</ul>
<h2>5. The environment</h2>
<p>Content about the environmental harms and benefits of AI.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.theguardian.com/us-news/2025/dec/08/us-data-centers">The Guardian - More than 200 environmental groups demand halt to new US datacenters</a></li>
<li><a href="https://www.theguardian.com/technology/2025/dec/18/2025-ai-boom-huge-co2-emissions-use-water-research-finds">The Guardian - AI boom has caused same CO2 emissions in 2025 as New York City, report claims</a></li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.ted.com/talks/manoush_zomorodi_amen_ra_mashariki_how_to_make_ai_a_force_for_good_in_climate">TED Talks Daily: How to make AI a force for good in climate | Amen Ra Mashariki and Manoush Zomorodi</a> (Dec 2025)</li>
</ul>
<h2>6. Regulations and legality</h2>
<p>Content about existing and potential regulations and issues of legality.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.theguardian.com/technology/2025/dec/16/boost-for-artists-in-ai-copyright-battle-as-only-3-per-cent-back-uk-active-opt-out-plan">The Guardian - Boost for artists in AI copyright battle as only 3% back UK active opt-out plan</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://blog.startupstash.com/github-copilot-litigation-a-deep-dive-into-the-legal-battle-over-ai-code-generation-e37cd06ed11c">GitHub Copilot Litigation: A Deep Dive Into The Legal Battle Over AI Code Generation</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">EU AI Act: first regulation on artificial intelligence</a></li>
<li><a href="https://gdprlocal.com/uk-ai-act/">UK AI Regulation: Current Status and Outlook</a></li>
</ul>
<h2>7. AI potential consciousness and welfare</h2>
<p>Content about AI systems themselves and whether or not we have any moral duty to treat them a certain way.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://ai-frontiers.org/articles/the-evidence-for-ai-consciousness-today">The evidence for AI consciousness today</a></li>
<li><a href="https://www.cam.ac.uk/research/news/we-may-never-be-able-to-tell-if-ai-becomes-conscious-argues-philosopher">We may never be able to tell if AI becomes conscious, argues philosopher</a></li>
<li><a href="https://www.theguardian.com/technology/2025/dec/30/ai-pull-plug-pioneer-technology-rights">The Guardian - AI showing signs of self-preservation and humans should be ready to pull plug, says pioneer</a> (“People demanding that AIs have rights would be a huge mistake”)</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=I9aGC6Ui3eE">Anthropic’s philosopher answers your questions</a> (Amanda Askell interview)</li>
</ul>
<h2>8. Open source models and alternative AI providers</h2>
<p>Exploring potentially more ethical options, away from the Big Tech AGI arms race. <em>Note</em>: this is more like a “to do” list of potential options at this stage, rather than tried-and-tested recommendations.</p>
<ul>
<li><a href="https://huggingface.co/docs/inference-providers/integrations/vscode">Hugging Face Visual Studio Code extension</a> - for open source LLMs</li>
<li><a href="https://medium.com/@walterdeane/setting-up-a-local-llm-for-code-assistance-e4c25afb5757">Setting Up a Local LLM for Code Assistance</a></li>
<li><a href="https://getstream.io/blog/best-local-llm-tools/">The 6 Best LLM Tools To Run Models Locally</a>]</li>
<li><a href="https://vscodium.com/">VS Codium</a> - community-driven, open source version of Microsoft’s VS Code editor</li>
</ul>
<p>--</p>
<p>Thumbnail image credit: <a href="https://flickr.com/photos/zoethustra/450139090/">Zoe Brown</a>, CC BY 2.0</p>

  </article>
</div>
    <footer>
      <p><a rel="me" href="https://toot.cafe/@peter">Mastodon</a> | <a href="https://github.com/poshaughnessy/">Github</a> | <a href="http://uk.linkedin.com/in/poshaughnessy/">LinkedIn</a> | <a href="https://github.com/poshaughnessy/peteroshaughnessy.com">Source</a> | <a href="rss.xml">RSS</a></p>
      <p>© Peter O'Shaughnessy</p>
    </footer>
  </body>
</html>
