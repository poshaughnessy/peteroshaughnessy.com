<!doctype html>
<html lang="en">
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>AI trajectory, ethics and safety resources - Peter O'Shaughnessy</title>
    <meta name="description" content="Web developer especially interested in web APIs and the future of the web">
    <link rel="stylesheet" href="/styles/styles.css"/>
    <link rel="icon" href="/images/favicon.png" type="image/png">
    <link rel="alternate" type="application/rss+xml" title="RSS feed" href="https://peteroshaughnessy.com/rss.xml" />
    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@poshaughnessy">
    <meta name="twitter:creator" content="@poshaughnessy">
      <meta name="twitter:title" content="AI trajectory, ethics and safety resources">
      <meta name="twitter:description" content="What I&#x27;ve been reading, listening to and watching, to understand AI&#x27;s dangers and what to do about it">
      <meta name="twitter:image" content="https://peteroshaughnessy.com/images/posts/2025-12-31-ai-ethics-safety-resources/thumb.jpg">
  </head>
  <body>
    <header>
      <div>
        <div class="avatar-container">
          <a href="/"><img class="avatar" src="/images/peter-cartoon-circle.png" alt="Peter's cartoon avatar"/></a>
        </div>
        <div class="title-and-tagline-container">
          <h1><a href="/">Peter O'Shaughnessy</a></h1>
        </div>
      </div>
      <nav>
        <ul>
          <li><a href="/">Blog</a></li>
          <li><a href="/projects/">Projects</a></li>
          <li><a href="/talks/">Talks</a></li>
          <li><a href="/about/">About</a></li>
        </ul>
      </nav>
    </header>
<div class="contents">
  <article class="page page-post">
    <h1>AI trajectory, ethics and safety resources</h1>
    <p class="date"><time>31st Dec 2025</time></p>
    <h2>What I&#39;ve been reading, listening to and watching, to understand AI&#39;s dangers and what to do about it</h2>
<p>Reading the <a href="https://medium.com/@ZombieCodeKill">blog posts from my brother Kevin</a> who is studying for an MSc in AI, I have recently become very concerned about the &quot;<a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/01/its-going-much-too-fast-the-inside-story-of-the-race-to-create-the-ultimate-ai">race to AGI</a>&quot; (Artificial General Intelligence).</p>
<p>I&#39;m worried about advanced AI <a href="https://www.authoritarian-stack.info/">concentrating power for authoritarians</a>. I&#39;m worried about <a href="https://www.ippr.org/media-office/up-to-8-million-uk-jobs-at-risk-from-ai-unless-government-acts-finds-ippr">mass job losses</a> without <a href="https://www.theguardian.com/business/2025/dec/15/universal-basic-income-ai-andrew-yang">a suitable safety net</a>. And I&#39;m worried about the <a href="https://www.thecompendium.ai/">existential risk</a> of superintelligence, if or when it arrives.</p>
<p>To try to understand the situation better and find ideas for what I might be able to do to help, I have spent the past couple of months trying to catch up by reading, watching and listening to <em>lots and lots</em> on AI: how it works, its capabilities and trajectory, near-term safety risks, ethical concerns, the risks of superintelligence and more.</p>
<p>Particularly to help share with colleagues in an Ethics and Sustainability community I co-organise, I have collected a lot of links. I thought I would share them here too, to help anyone else wanting to &quot;speed run&quot; these topics. As <a href="www.ted.com/talks/tristan_harris_why_ai_is_our_ultimate_test_and_greatest_invitation">Tristan Harris says</a>, the best thing we can do to help right now is to try to increase public awareness and clarity.</p>
<p>Given the links are broadly about ethics and safety, there is naturally more of a focus on the dangers of AI. However, I have also been mindful to try to include some counter-arguments and differing views, to help paint a fuller picture - particularly with regard to potential AGI timelines.</p>
<p>I will try to update the list every now and then. Please do not view it as a comprehensive set of topics, but just as a starting point. If you&#39;re hoping to catch up like me, or just looking for anything you might have missed, I hope you might find it useful.</p>
<h2>Contents</h2>
<ol>
  <li><a href="#capability">AI capability and trajectory</a></li>
  <li><a href="#near-term">Current and near-term AI safety risks and ethics</a></li>
  <li><a href="#superintelligence">Superintelligence risks</a></li>
  <li><a href="#jobs">Job replacement</a></li>
  <li><a href="#environment">The environment</a></li>
  <li><a href="#regulations">Regulations and legality</a></li>
  <li><a href="#welfare">AI potential consciousness and welfare</a></li>
  <li><a href="#alternatives">Open source models and alternative AI providers</a></li>
</ol>

<h2 id="capability">1. AI capability and trajectory</h2>

<p>Content to help understand how AI is advancing and how near or far we might be from Artifcial General Inteligence (AGI).</p>
<p><em>Note:</em> Alternative terms for AGI include “powerful AI” (<a href="https://www.darioamodei.com/essay/machines-of-loving-grace#basic-assumptions-and-framework">Dario Amodei’s preferred term</a>) and “Higher Level Machine Intelligence” (HLMI, used by the <a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai">ESPAI survey</a>, for <em>“when unaided machines can accomplish every task better and more cheaply than human workers”</em>). This seems essentially equivalent to the term “strong AGI” used in the <a href="https://asteriskmag.substack.com/p/common-ground-between-ai-2027-and">Common Ground between AI 2027 &amp; AI as Normal Technology</a> essay (<em>&quot;AI systems which can... do virtually everything humans can do just as well but faster and cheaper&quot;</em>).</p>
<h3>Articles</h3>
<ul>
<li><a href="https://time.com/6300942/ai-progress-charts/">TIME - 4 Charts That Show Why AI Progress Is Unlikely to Slow Down</a> (Nov 2023)</li>
<li><a href="https://edition.cnn.com/2025/10/18/business/ai-bubble-analyst-nightcap">CNN - Why this analyst says the AI bubble is 17 times bigger than the dot-com bust</a> (Oct 2025)</li>
<li><a href="https://asteriskmag.substack.com/p/common-ground-between-ai-2027-and">Common Ground between AI 2027 &amp; AI as Normal Technology</a> (Nov 2025)</li>
<li><a href="https://www.theguardian.com/technology/2025/dec/19/data-centers-ai-investment">The Guardian - Investment in data centers worldwide hit record $61bn in 2025, report finds</a> (Dec 2025)<ul>
<li><em>“This month, a trio of analysts from Deutsche Bank put out a staggering chart indicating that OpenAI alone will burn through $143bn between 2024 and 2029, the year before the company claims it will turn a profit.”</em></li>
</ul>
</li>
<li><a href="https://www.theguardian.com/commentisfree/2025/dec/23/artificial-intelligence-ai-bubble-bursts-humans-take-back-control">The Guardian - When the AI bubble bursts, humans will finally have their chance to take back control</a> (Dec 2025)<ul>
<li><em>“They have built spectacular engines that prioritise a brilliant performance of intelligence over the real thing.”</em></li>
</ul>
</li>
<li><a href="https://www.ft.com/content/728b03a4-cef3-4ee9-a421-d681998ef7d8">FT - AI upheaval shows little sign of lessening</a> (Dec 2025, paywalled)<ul>
<li>The amount of machine intelligence that can be purchased for a dollar is growing around 10x every year</li>
</ul>
</li>
<li><a href="https://www.scientificamerican.com/article/how-close-are-todays-ai-models-to-agi-and-to-self-improving-into/">Scientific American - Are We Seeing the First Steps Toward AI Superintelligence?</a> (Dec 2025)<ul>
<li><em>“...there’s no question that some AI systems have become capable of discovering solutions humans had not previously found.”</em></li>
</ul>
</li>
<li><a href="https://www.nbcnews.com/tech/innovation/andrew-ng-says-ai-limited-wont-replace-humans-anytime-soon-rcna246074">NBC - An AI pioneer [Andrew Ng] says the technology is &#39;limited&#39; and won&#39;t replace humans anytime soon</a> (Dec 2025)</li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://medium.com/@clairedigitalogy/llms-gans-and-diffusion-models-breaking-down-ai-jargon-80ffd54e83ff">LLMs, GANs, and Diffusion Models: Breaking Down AI Jargon</a> (Apr 2025)</li>
<li><a href="https://medium.com/@sanderink.ursina/jepa-a-family-of-predictive-architectures-for-human-like-ai-ae9b55e92dc7">JEPA: A Family of Predictive Architectures for Human-Like AI</a> (Apr 2025)</li>
<li><a href="https://yosefk.com/blog/llms-arent-world-models.html">LLMs aren&#39;t world models</a> (Aug 2025)<ul>
<li><em>“LLMs are not by themselves sufficient as a path to general machine intelligence”</em></li>
</ul>
</li>
<li><a href="https://thezvi.substack.com/p/yes-ai-continues-to-make-rapid-progress">Zvi Mowshowitz - Yes, AI Continues To Make Rapid Progress, Including Towards AGI</a> (Sep 2025)</li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://wiki.aiimpacts.org/ai_timelines/predictions_of_human-level_ai_timelines/ai_timeline_surveys/2023_expert_survey_on_progress_in_ai">2023 Expert Survey on Progress in AI (ESPAI)</a><ul>
<li><em>“The resulting aggregate forecast gives a 50% chance of HLMI [Higher Level Machine Intelligence] by 2047, down thirteen years from 2060 in the 2022 ESPAI”</em></li>
</ul>
</li>
<li><a href="https://www.metaculus.com/questions/5121/date-of-artificial-general-intelligence/">Metaculus - Date of Artificial General Intelligence</a><ul>
<li>Online forecasting platform, current estimate: June 2033 (recorded Dec 2025), based on 1.8k forecasters</li>
</ul>
</li>
<li><a href="https://theagiclock.com/">The AGI Clock</a> (aggregated expert predictions)<ul>
<li>Industry leaders: 2029, Research community: 2036</li>
</ul>
</li>
<li><a href="https://metr.org/blog/2025-03-19-measuring-ai-ability-to-complete-long-tasks/">Metr - Measuring AI Ability to Complete Long Tasks</a> (exponential improvement at 50% reliability)</li>
<li><a href="https://arxiv.org/pdf/2510.18212">A Definition of AGI</a> (paper)</li>
<li><a href="https://research.aimultiple.com/ai-hallucination/">AI Hallucination: Compare top LLMs</a> (Dec 2025)<ul>
<li><em>“there is little to no linear correlation between context capacity and accuracy”</em></li>
</ul>
</li>
<li><a href="https://www.lakera.ai/blog/guide-to-hallucinations-in-large-language-models">LLM Hallucinations in 2025</a><ul>
<li><em>“Studies... found that even well-curated retrieval pipelines can fabricate citations.”</em></li>
</ul>
</li>
<li><a href="https://www.oreilly.com/radar/what-if-ai-in-2026-and-beyond/">O&#39;Reilly - What If? AI in 2026 and Beyond</a><ul>
<li>Discusses the two fundamental scenarios: “AI is an economic singularity” or “AI is a normal technology” and how to spot signs suggesting which scenario is correct</li>
</ul>
</li>
<li><a href="https://epoch.ai/blog/what-will-ai-look-like-in-2030">Epoch AI - What will AI look like in 2030?</a></li>
<li><a href="https://80000hours.org/agi/guide/when-will-agi-arrive/">The case for AGI by 2030</a><ul>
<li><em>“Over and over again, it’s been dangerous to bet against deep learning...Given the massive implications and serious risks, there’s enough evidence to take this possibility [of AGI] extremely seriously.”</em></li>
</ul>
</li>
<li><a href="https://www.gov.uk/government/publications/frontier-ai-capabilities-and-risks-discussion-paper">Frontier AI: capabilities and risks</a> - UK government research, Oct 2023</li>
<li><a href="https://www.anthropic.com/research/introspection">Anthropic - Signs of introspection in large language models</a><ul>
<li>Introspective awareness <em>“might facilitate more advanced forms of deception or scheming”</em>. The latest models demonstrated the greatest introspection awareness, suggesting it might scale with general capability. <em>“As this capability improves, models might become increasingly resistant to oversight, safety interventions, or course corrections — because they can tell when something external is constraining them.”</em> (<a href="https://medium.com/@ZombieCodeKill/claude-on-feeling-and-introspection-bc732c0a206b">source</a>)</li>
</ul>
</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.dwarkesh.com/p/andrej-karpathy">Dwarkesh Podcast - Andrej Karpathy - AGI is still a decade away</a> (Oct 2025)<ul>
<li>Andrej Karpathy is a former OpenAI co-founder</li>
</ul>
</li>
<li><a href="https://youtu.be/aR20FWCCjAs">Dwarkesh Podcast - Ilya Sutskever – We&#39;re moving from the age of scaling to the age of research</a> (Nov 2025)<ul>
<li>Ilya is the former co-founder and chief scientist of OpenAI and now the co-founder of Safe Superintelligence (SSI)</li>
</ul>
</li>
<li><a href="https://youtu.be/HcyUUvxkykE">The AI Daily Brief: 51 Charts That Will Shape AI in 2026</a> (Dec 2025)<ul>
<li><em>“There’s no common definition of AGI and there are even plenty of folks out there who think that the term is getting more and more meaningless... If anything, I think people&#39;s [AGI] timelines got moved back slightly heading into 2026 from where they were heading into 2025...”</em></li>
</ul>
</li>
<li><a href="https://paulkrugman.substack.com/p/talking-with-paul-kedrosky">Paul Krugman - Talking With Paul Kedrosky</a> (Dec 2025) (on the economics and signs of a bubble)<ul>
<li><em>“...you’re seeing people increasingly look at other approaches. I think in all likelihood, none of them will lead to anything like AGI.”</em></li>
</ul>
</li>
</ul>
<h3>Quotes</h3>
<ul>
<li><em>&quot;Considering that our intelligence is fixed and machine intelligence is growing, it is only a matter of time before machines surpass us...&quot;</em><ul>
<li><a href="https://research.aimultiple.com/artificial-general-intelligence-singularity-timing/#why-experts-believe-agi-is-inevitable-key-arguments-evidence">AIMultiple - Why experts believe AGI is inevitable</a></li>
</ul>
</li>
<li><em>&quot;What I know from the technical people I speak to here in Silicon Valley is the vast majority of them are very dubious that we are even on the path towards some kind of superintelligent, sentient being. The vast majority of them think that the current moment we&#39;re in with LLMs... doesn&#39;t even put us on the track to that sort of stuff...&quot;</em><ul>
<li>Jacob Ward, journalist and author (CNN, PBS, Al Jazeera, The Loop) on the <a href="https://theairisknetwork.substack.com/p/what-we-lose-when-ai-makes-choices">For Humanity podcast</a></li>
</ul>
</li>
</ul>
<h2 id="near-term">2. Current and near-term AI safety risks and ethics</h2>

<p>Content to help understand near-term harms and risks from AI.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.rollingstone.com/culture/culture-features/women-warnings-ai-danger-risk-before-chatgpt-1234804367/">Rolling Stone - These Women Tried To Warn Us About AI</a> (Aug 2023)<ul>
<li>About bias in AI and the tendency for those worried about the existential threat to be people who don&#39;t suffer structural inequality</li>
</ul>
</li>
<li><a href="https://www.rollingstone.com/culture/culture-features/spiralist-cult-ai-chatbot-1235463175/">Rolling Stone - This Spiral-Obsessed AI ‘Cult’ Spreads Mystical Delusions Through Chatbots</a> (Nov 2025)<ul>
<li>Based on <a href="https://www.lesswrong.com/posts/6ZnznCaTcbGYsCmqu/the-rise-of-parasitic-ai">this disturbing research by Adele Lopez</a></li>
</ul>
</li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.anthropic.com/research/agentic-misalignment">Anthropic - Agentic misalignment</a><ul>
<li><em>“In at least some cases, models from all developers resorted to malicious insider behaviors when that was the only way to avoid replacement or achieve their goals—including blackmailing officials and leaking sensitive information to competitors”</em></li>
</ul>
</li>
<li><a href="https://www.gov.uk/government/collections/responsible-ai-toolkit">Gov.UK Responsible AI Toolkit</a></li>
<li><a href="https://www.aisi.gov.uk/">AI Security Institute</a> (UK government department)</li>
<li><a href="https://futureoflife.org/ai-safety-index-winter-2025/">Future of Life Institute - AI Safety Index - Winter 2025</a></li>
<li><a href="https://www.dair-institute.org/">Distributed AI Research (DAIR) Institute</a> (founded by Timnit Gebru to “counter Big Tech’s pervasive influence on AI research, development and deployment”)</li>
<li><a href="https://partnershiponai.org/">Partnership on AI (PAI)</a> (non-profit for positive AI outcomes)</li>
<li><a href="https://humane-intelligence.org/">Humane Intelligence</a> (non-profit for making AI “more accountable, responsible, and fair”)</li>
<li><a href="https://civai.org/">CivAI (Civic AI Security Program)</a> (non-profit that educates about AI&#39;s capabilities and dangers using live demonstrations)</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://youtu.be/yZtVuo07j7o">UK Deputy PM David Lammy&#39;s address to the UN Security Council on AI threat</a> (Sep 2025)</li>
<li><a href="https://www.dair-institute.org/maiht3k/">Mystery AI Hype Theater 3000</a> (AI ethics and anti AI hype podcast)</li>
<li><a href="https://youtu.be/vPaF6TypfX8">BBC News - How hackers are using AI and how to protect yourself</a> (Nov 2025)<ul>
<li><em>“This is going to be the most tricky technology to secure that we&#39;ve ever created... The places that are doing this well are understandably cautious and they are building one small step with a whole load of defensive controls around that...”</em></li>
</ul>
</li>
</ul>
<h2 id="superintelligence">3. Superintelligence risks</h2>

<p>Content about the risk of artificial superintelligence (ASI).</p>
<h3>Petitions / Statements</h3>
<ul>
<li><a href="https://aistatement.com/">Statement on AI Risk</a> (please consider signing)</li>
<li><a href="https://superintelligence-statement.org/">Statement on Superintelligence</a> (please consider signing)</li>
<li><a href="https://controlai.com/statement">ControlAI Statement</a> (UK, please consider sending to your MP)</li>
</ul>
<h3>Articles</h3>
<ul>
<li><a href="https://www.bbc.co.uk/news/uk-65746524">BBC News - Artificial intelligence could lead to extinction, experts warn</a> (May 2025)</li>
<li><a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/02/jared-kaplan-artificial-intelligence-train-itself">The Guardian - ‘The biggest decision yet’: Jared Kaplan on allowing AI to train itself</a> (Dec 2025)</li>
<li><a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/01/its-going-much-too-fast-the-inside-story-of-the-race-to-create-the-ultimate-ai">The Guardian - ‘It’s going much too fast’: the inside story of the race to create the ultimate AI</a> (Dec 2025)</li>
<li><a href="https://www.theguardian.com/technology/ng-interactive/2025/dec/30/the-office-block-where-ai-doomers-gather-to-predict-the-apocalypse">The Guardian - The office block where AI ‘doomers’ gather to predict the apocalypse</a> (Dec 2025)</li>
<li>Counter-argument: <a href="https://www.noemamag.com/the-politics-of-superintelligence/">The Politics of Superintelligence</a> (Dec 2025)</li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.narrowpath.co/">Narrow Path</a></li>
<li><a href="https://keepthefuturehuman.ai/">Keep The Future Human</a></li>
<li><a href="https://aisafety.info/">AISafety.info</a></li>
<li><a href="https://controlai.com/">ControlAI</a> (UK and global campaign)</li>
<li><a href="https://www.thecompendium.ai/">The Compendium</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-1.html">Wait But Why - The AI Revolution: The Road to Superintelligence</a> (2015)</li>
<li><a href="https://waitbutwhy.com/2015/01/artificial-intelligence-revolution-2.html">Wait But Why - The AI Revolution: Our Immortality or Extinction</a> (2015)</li>
<li><a href="https://blog.samaltman.com/machine-intelligence-part-1">Sam Altman - Machine Intelligence Part 1</a> (2015)</li>
<li><a href="https://blog.samaltman.com/machine-intelligence-part-2">Sam Altman - Machine Intelligence Part 2</a> (2015)</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.ted.com/talks/tristan_harris_why_ai_is_our_ultimate_test_and_greatest_invitation">TED Talk - Tristan Harris - Why AI is our ultimate test and greatest invitation</a> (Apr 2025) (written version <a href="https://centerforhumanetechnology.substack.com/p/the-narrow-path-why-ai-is-our-ultimate">here</a>)</li>
<li><a href="https://youtu.be/BFU1OCkhBwo">Diary of a CEO - AI Expert: We Have 2 Years Before Everything Changes - Tristan Harris</a> (Nov 2025) (please try to look past the scary Terminator style preview image, it&#39;s a bit sensationalist but I think very good otherwise) (podcast version <a href="https://episode.flightcast.com/01KB0GWEH63PB47R8VEVSN95KW.mp3">here</a>)</li>
<li><a href="https://your-undivided-attention.simplecast.com/episodes/feed-drop-into-the-machine-with-tobias-rose-stockwell-DghO_Lhb">Your Undivided Attention - Feed Drop: &quot;Into the Machine&quot; with Tobias Rose-Stockwell and Tristan Harris</a></li>
<li><a href="https://podcast.futureoflife.org/">Future of Life Institute Podcast</a></li>
<li><a href="https://theairisknetwork.substack.com/s/for-humanity-an-ai-risk-podcast">For Humanity: An AI Risk Podcast</a></li>
<li><a href="https://www.longviewinvestigations.com/">The Last Invention Podcast</a></li>
</ul>
<h3>Books</h3>
<ul>
<li><a href="https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies">Nick Bostrom - Superintelligence</a> (2014)</li>
</ul>
<h3>Quotes</h3>
<ul>
<li><em>“Development of superhuman machine intelligence is probably the greatest threat to the continued existence of humanity.”</em><ul>
<li>Sam Altman in his <a href="https://blog.samaltman.com/machine-intelligence-part-1">2015 blog post</a>.</li>
</ul>
</li>
<li><em>“AI CEOs claim they know how to build superhuman AI, yet none can show how they&#39;ll prevent us from losing control – after which humanity&#39;s survival is no longer in our hands. I&#39;m looking for proof that they can reduce the annual risk of control loss to one in a hundred million, in line with nuclear reactor requirements. Instead, they admit the risk could be one in ten, one in five, even one in three, and they can neither justify nor improve those numbers.”</em><ul>
<li>Prof. Stuart Russell, Professor of Computer Science at UC Berkeley</li>
</ul>
</li>
<li><em>&quot;We&#39;re currently releasing the most powerful, inscrutable, uncontrollable technology we&#39;ve ever invented, that&#39;s already demonstrating behaviours of self-preservation and deception... We&#39;re releasing it faster than we&#39;ve released any other technology in history, and with the maximum incentive to cut corners on safety.&quot;</em><ul>
<li>Tristan Harris, Co-Founder, Center for Humane Tech</li>
</ul>
</li>
<li><em>“AI companies are racing to build technology that they absolutely do not understand and cannot control.”</em><ul>
<li>Steven Adler, former lead safety researcher at OpenAI</li>
</ul>
</li>
</ul>
<h2 id="jobs">4. Job replacement</h2>

<p>Content about the impact on jobs as AI advances and potentially reaches AGI.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.axios.com/2025/05/28/ai-jobs-white-collar-unemployment-anthropic">Axios - Behind the Curtain: A white-collar bloodbath</a> (2025)</li>
<li><a href="https://www.theguardian.com/business/2025/dec/15/universal-basic-income-ai-andrew-yang">The Guardian - Why universal basic income still can’t meet the challenges of an AI economy</a></li>
<li><a href="https://restofworld.org/2025/engineering-graduates-ai-job-losses/">“Everyone is so panicked”: Entry-level tech workers describe the AI-fueled jobpocalypse</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://chrisbarber.co/I+asked+AI+researchers+and+economists+about+SWE+career+strategies+given+AI">I asked AI researchers and economists about SWE career strategies given AI</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.signalfire.com/blog/signalfire-state-of-talent-report-2025">SignalFire State of Tech Talent Report 2025</a> (&quot;New grad hiring drops 50%...&quot;)</li>
<li><a href="https://www.ippr.org/media-office/up-to-8-million-uk-jobs-at-risk-from-ai-unless-government-acts-finds-ippr">Institute for Public Policy Research (IPPR)</a> (&quot;8 million UK jobs at risk&quot;)</li>
<li><a href="https://www.stlouisfed.org/on-the-economy/2025/aug/is-ai-contributing-unemployment-evidence-occupational-variation">Is AI Contributing to Rising Unemployment? Evidence from Occupational Variation</a> (“occupations with higher AI exposure experienced larger unemployment rate increases”)</li>
</ul>
<h3>Quotes</h3>
<ul>
<li>AGI will be <em>“like a flood of millions of new digital immigrants [of] Nobel Prize level capability [who] work at superhuman speed [and] will work for less than minimum wage.“</em><ul>
<li>Tristan Harris</li>
</ul>
</li>
<li><em>&quot;You know, we had the elevator man, and now we have automated elevators. We had bank tellers, now we have automated teller machines. So humans will always just find something else to do. But why is AI different than that? Because it’s intelligence, because it’s general intelligence. That means that rather than a technology that automates just bank tellers, this is automating all forms of human cognitive labor, meaning everything that a human mind can do. So who’s going to retrain faster? You moving to that other kind of cognitive labor or the AI that is trained on everything and can multiply itself by 100 million times and is retraining how to do that kind of labor?&quot;</em><ul>
<li>Tristan Harris</li>
</ul>
</li>
<li>Counter-argument: <em>&quot;I think we live in a very complex, high-friction world where actually intelligence is so multi-dimensional and all this &#39;replacement&#39; is just incredibly hard and in the real world when you&#39;re at the coal face... and humans are just so versatile and so well evolved to deal with the world that we actually live in... I think [AI] will work extraordinarily well in domains that are extraordinarily valuable, but I do not think we&#39;re heading for this frictionless, plug-and-play, is-a-human replacement, so I think humans will have comparative advantage in a bunch of things for a very long time.&quot;</em><ul>
<li>Matt Clifford, ex government AI advisor, on the Rest Is Politics podcast</li>
</ul>
</li>
</ul>
<h2 id="environment">5. The environment</h2>

<p>Content about the environmental harms (and potential benefits) of AI.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.theguardian.com/us-news/2025/dec/08/us-data-centers">The Guardian - More than 200 environmental groups demand halt to new US datacenters</a></li>
<li><a href="https://www.theguardian.com/technology/2025/dec/18/2025-ai-boom-huge-co2-emissions-use-water-research-finds">The Guardian - AI boom has caused same CO2 emissions in 2025 as New York City, report claims</a></li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.ted.com/talks/manoush_zomorodi_amen_ra_mashariki_how_to_make_ai_a_force_for_good_in_climate">TED Talks Daily: How to make AI a force for good in climate | Amen Ra Mashariki and Manoush Zomorodi</a> (Dec 2025)</li>
</ul>
<h2 id="regulations">6. Regulations and legality</h2>

<p>Content about existing and potential regulations and issues of legality.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://www.theguardian.com/technology/2025/dec/16/boost-for-artists-in-ai-copyright-battle-as-only-3-per-cent-back-uk-active-opt-out-plan">The Guardian - Boost for artists in AI copyright battle as only 3% back UK active opt-out plan</a></li>
</ul>
<h3>Blog posts</h3>
<ul>
<li><a href="https://blog.startupstash.com/github-copilot-litigation-a-deep-dive-into-the-legal-battle-over-ai-code-generation-e37cd06ed11c">GitHub Copilot Litigation: A Deep Dive Into The Legal Battle Over AI Code Generation</a></li>
</ul>
<h3>Reports / Research / Resources</h3>
<ul>
<li><a href="https://www.europarl.europa.eu/topics/en/article/20230601STO93804/eu-ai-act-first-regulation-on-artificial-intelligence">EU AI Act: first regulation on artificial intelligence</a></li>
<li><a href="https://gdprlocal.com/uk-ai-act/">UK AI Regulation: Current Status and Outlook</a></li>
</ul>
<h2 id="welfare">7. AI potential consciousness and welfare</h2>

<p>Content about AI systems themselves and whether or not we have any moral duty to treat them a certain way.</p>
<h3>Articles</h3>
<ul>
<li><a href="https://ai-frontiers.org/articles/the-evidence-for-ai-consciousness-today">The evidence for AI consciousness today</a></li>
<li><a href="https://www.cam.ac.uk/research/news/we-may-never-be-able-to-tell-if-ai-becomes-conscious-argues-philosopher">We may never be able to tell if AI becomes conscious, argues philosopher</a></li>
<li><a href="https://www.theguardian.com/technology/2025/dec/30/ai-pull-plug-pioneer-technology-rights">The Guardian - AI showing signs of self-preservation and humans should be ready to pull plug, says pioneer</a> (“People demanding that AIs have rights would be a huge mistake”)</li>
</ul>
<h3>Podcasts / Videos</h3>
<ul>
<li><a href="https://www.youtube.com/watch?v=I9aGC6Ui3eE">Anthropic’s philosopher answers your questions</a> (Amanda Askell interview)</li>
</ul>
<h2 id="alternatives">Open source models and alternative AI providers</h2>

<p>Exploring potentially more ethical options, away from the Big Tech AGI arms race.</p>
<p><em>Note</em>: this is more like a “to do” list of potential options at this stage, rather than tried-and-tested recommendations.</p>
<ul>
<li><a href="https://huggingface.co/docs/inference-providers/integrations/vscode">Hugging Face Visual Studio Code extension</a> - for open source LLMs</li>
<li><a href="https://medium.com/@walterdeane/setting-up-a-local-llm-for-code-assistance-e4c25afb5757">Setting Up a Local LLM for Code Assistance</a></li>
<li><a href="https://getstream.io/blog/best-local-llm-tools/">The 6 Best LLM Tools To Run Models Locally</a></li>
<li><a href="https://vscodium.com/">VS Codium</a> - community-driven, open source version of Microsoft’s VS Code editor</li>
</ul>
<p>--</p>
<p><small>Thumbnail image credit: <a href="https://flickr.com/photos/zoethustra/450139090/">Zoe Brown</a>, CC BY 2.0</small></p>

  </article>
</div>
    <footer>
      <p><a rel="me" href="https://toot.cafe/@peter">Mastodon</a> | <a href="https://github.com/poshaughnessy/">Github</a> | <a href="http://uk.linkedin.com/in/poshaughnessy/">LinkedIn</a> | <a href="https://github.com/poshaughnessy/peteroshaughnessy.com">Source</a> | <a href="rss.xml">RSS</a></p>
      <p>© Peter O'Shaughnessy</p>
    </footer>
  </body>
</html>
