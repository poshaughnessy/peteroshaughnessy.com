<!doctype html>
<html>
  <head>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1">
      <title>Interfaces of the future, and how to hack around with them now - Peter O'Shaughnessy</title>
    <meta name="description" content="Web developer especially interested in web APIs and the future of the web">
    <link rel="stylesheet" href="/styles/styles.css"/>
    <link rel="icon" href="/images/favicon.png" type="image/png">
    <link rel="alternate" type="application/rss+xml" title="RSS feed" href="https://peteroshaughnessy.com/rss.xml" />
    <!-- Twitter Card data -->
    <meta name="twitter:card" content="summary">
    <meta name="twitter:site" content="@poshaughnessy">
    <meta name="twitter:creator" content="@poshaughnessy">
      <meta name="twitter:title" content="Interfaces of the future, and how to hack around with them now">
      <meta name="twitter:description" content="The Web is moving out from behind the screen...">
      <meta name="twitter:image" content="https://peteroshaughnessy.com/images/posts/2014-08-04-interfaces-of-the-future/thumb-pictures-under-glass.jpg">
  </head>
  <body>
    <header>
      <div>
        <div class="avatar-container">
          <a href="/"><img class="avatar" src="/images/peter-cartoon-circle.png" alt="Peter's cartoon avatar"/></a>
        </div>
        <div class="title-and-tagline-container">
          <h1><a href="/">Peter O'Shaughnessy</a></h1>
          <h2>Web technologies and browser-based experiments</h2>
        </div>
      </div>
      <nav>
        <ul>
          <li><a href="/">Blog</a></li>
          <li><a href="/projects/">Projects</a></li>
          <li><a href="/talks/">Talks</a></li>
          <li><a href="/about/">About</a></li>
        </ul>
      </nav>
    </header>
<div class="contents">
  <article class="page page-post">
    <h1>Interfaces of the future, and how to hack around with them now</h1>
    <p class="date"><time>4th Aug 2014</time></p>
    <p><em>On 31st July I gave the following talk at <a href="http://www.frontendlondon.co.uk/">Front End London</a>. The <a href="http://poshaughnessy.github.io/fel-interfaces-of-the-future/">slides are
here</a>.</em></p>
<p>I’d like to talk today about some of the new kinds of interfaces that
are on the horizon and may be taking off in the next few years. And for
a couple of the devices that I’m finding most exciting at the moment,
I’ll introduce you to hacking around with them right now, using Web
technologies.</p>
<p>Luckily I get to explore this kind of stuff in my day job, as a
Developer in the <a href="http://labs.pearson.com/prototypes/welcome-to-the-future-technologies-prototypes-page/">Future Technologies team in
Pearson</a>,
the world’s leading learning company. (Some of you here may know it as
the parent company of the Financial Times).</p>
<p>First, let’s think about where we are now, in the world of the Web.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/devices.jpg" alt="Cuddling with multiple devices"></p>
<p class="caption">Credit: <a href="http://en.wikipedia.org/wiki/Digital_omnivore#mediaviewer/File:Cuddling_with_multiple_devices.jpg">Jeremy Keith</a></p>

<p>We’ve moved on from the Desktop Era, and having travelled through
the Age of Mobile, we’re now in the Multi-Device Era, where we no longer
have neat categories of devices (“smartphone”, “tablet”, “desktop”), but
a continuum of different screen sizes and an assortment of touch-screens
and non-touch screens.</p>
<p>But in a way, all these devices are still kind of the same. They’re all
flat, 2D screens that we have in our hands, or just in front of us. As
Bret Victor memorably said, it’s all just <a href="http://worrydream.com/ABriefRantOnTheFutureOfInteractionDesign/">Pictures Under
Glass</a>.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/pictures-under-glass.jpg" alt="Pictures Under Glass"></p>
<p class="caption">Credit: <a href="https://placeit.net/">PlaceIt</a></p>

<p>That surely can’t be it… So what’s next?</p>
<p>Well I’m sure we’ve all been hearing a lot of hype about wearables
recently. Interest in smartwatches has surged since 2012 when Pebble (on
the left here) became the first $10m Kickstarter campaign.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/smartwatches.jpg" alt="Smartwatches"></p>
<p class="caption">Credit: <a href="https://www.flickr.com/photos/janitors/10345415843/">Kārlis Dambrāns</a></p>

<p>Now Google, Samsung, LG, Motorola and many others are getting in on the
act, and of course we’re all waiting to see what Apple may or may not
reveal later this year.</p>
<p>And who could forget the poster child of geeky new tech, Google Glass?</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/google-glass-geeky.jpg" alt="Google Glass"></p>
<p class="caption">Credit: <a href="https://www.flickr.com/photos/thomashawk/14259298346">Thomas Hawk</a></p>

<p>If you haven’t already tried it out, I’m sure you’ve all heard
lots of opinions about it already.</p>
<p>You might be, quite rightly, feeling a bit skeptical about these
wearable devices taking off. Are people really going to want to use
them? Are they actually useful? How many people would actually be happy
to wear things like this out in public? And you might be thinking that
they aren’t really that different to what we have now. Even if they do
become popular, aren’t they merely additions to the Multi-device Era
we’re already in?</p>
<p>I think those are all very reasonable thoughts, when looking at
the wearables space right now.</p>
<p>But I’d like to offer up a couple of reasons why I think that
certain kinds of wearable devices could become a very big deal in the
near-future…</p>
<p><strong>Different devices, different experiences</strong></p>
<p>The first thing I’d like to say is that, as with smartphones and
tablets, the differences in these devices aren’t merely in the sizes of
the screens. It’s in how we use them and the different kinds of
experiences that they lend themselves to. For example, when the iPad was
announced, a lot of people dismissed it as just being a “big phone”. Too
big to take out with us all the time, and why would we need them at home
because we already have our laptop there? But it turned out that they
can make for a great “lean back” device, something that we’re more
comfortable using on the sofa.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/lean-back-experience.jpg" alt="Lean back experiences"></p>
<p class="caption">Credit: <a href="https://www.flickr.com/photos/plantronicsgermany/8071203642">plantronicsgermany</a></p>

<p>So I think we should think carefully about the kinds of experiences that
new devices might lend themselves to as well.</p>
<p>Especially because some of the ways that we end up interacting with new
technology are often difficult to predict. Before the smartphone
explosion, who would have predicted that so many of us would use them
for this:</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/selfie.jpg" alt="Selfie"></p>
<p class="caption">The “Selfie”. Credit: <a href="http://en.wikipedia.org/wiki/Selfie">Wikimedia</a></p>

<p><strong>The Long Nose of Innovation</strong></p>
<p>And I’d like to talk about being patient… Disruptive technology
doesn’t take off as soon as it’s been invented. New types of devices
start off in research labs as big, clunky, expensive things. Then they
go through years of refinement and augmentation until eventually
everything comes together: affordability, ease of use, good marketing…
and finally they can take off and gain traction.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/long-nose-of-innovation.png" alt="Long Nose of Innovation"></p>
<p class="caption">Credit: <a href="http://www.sketchplanations.com/post/60700964916/the-long-nose-of-innovation-bill-buxton-i-love">Sketchplanations</a></p>

<p>We’ve seen this many times over the years. For example, multi-touch
interfaces have been around in some form for decades, but didn’t really
take off until the iPhone. And it’s the same story with tablet computers
and the iPad.</p>
<p>And we all kind of know this, yet we still seem to go through this hype
curve every time:</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/hype-curve.png" alt="Hype Curve"></p>
<p class="caption">Credit: <a href="http://en.wikipedia.org/wiki/Hype_cycle">Wikipedia</a></p>

<p>I’m guilty of this too. We hear about a cool new technology, and we get
really excited about it, and then we try it, and it lets us down. It
doesn’t meet our expectations, and our instant reaction is that the
whole thing is a load of rubbish. But gradually as the tech becomes more
refined, we start to understand more about what they’re good for and
what they’re bad for, and eventually they just become another part of
everyday life.</p>
<p>So bearing all this in mind, let’s take a look at a couple of upcoming
paradigms that I think could be genuinely disruptive in the next few
years…</p>
<p><strong>Augmented Reality and Holographics</strong></p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/augmented-reality-holographics.jpg" alt="Augmented Reality"></p>
<p class="caption">Credit: <a href="http://youtu.be/yQRdIZR_LYY">Pearson School of Thought</a></p>

<p>Firstly, holographic-style augmented reality interfaces, where you can
reach out, create and interact with virtual content in 3D, Tony Stark
-style. Basically, future AR displays combined with future Leap
Motion-style sensors to let you manipulate things with your hands in
natural ways. This could bring the real world and the digital world a
lot closer together. Imagine using this to collaborate with people to
design and create things - each being able to see what each other is
painting in the air…</p>
<p>It might seem like this is still a long way off…But it shouldn’t be long
before we can start to try it out at least. Meta are planning to bring
“the first holographic interface” to market next year.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/meta-pro.jpg" alt="Meta Pro glasses"></p>
<p class="caption">Credit: <a href="https://www.spaceglasses.com">Meta</a></p>

<p>This Pro version will be $3,650 and it’ll be attached to a pocket
computer. So there’s a couple of reasons already why it’s unlikely to
shoot up that traction axis straight away. But some smart people predict
that it will only be 5 years before it becomes this:</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/ray-bans.jpg" alt="Ray Bans"></p>
<p class="caption">Credit: <a href="http://upload.wikimedia.org/wikipedia/commons/4/4b/RayBanAviator.jpg">Wikimedia</a></p>

<p>Just regular looking glasses or shades. That could really help to open
it up to the mass market.</p>
<p><strong>Virtual Reality</strong></p>
<p>And how about Virtual Reality?</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/matrix.jpg" alt="The Matrix"></p>
<p class="caption">Credit: <a href="http://www.flickr.com/photos/sudhee/82891943">Sudhee</a></p>

<p>Again, people have been talking about it for decades. But it should only
be next year before we have the first affordable Virtual Reality
consumer devices go on sale, like the Oculus Rift:</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/oculus-rift.jpg" alt="Oculus Rift"></p>
<p class="caption">Credit: <a href="http://www.oculusvr.com/">Oculus Rift</a></p>

<p>The unique thing about VR is the feeling of “presence”; you’re
transported into another environment. Go to the edge of a cliff in
virtual reality and you should find that you get sweaty palms and a
quickening heart beat, like you would in real life.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/presence-cliff.jpg" alt="Feet hanging off a cliff"></p>
<p class="caption">Credit: <a href="http://www.fotolia.com/">Fotolia</a></p>

<p>Of course, your conscious brain knows that you’re just wearing what is
effectively a pair of clunky ski goggles. But enough of your
subconscious brain is tricked that it can feel like you’re actually
immersed in another world…</p>
<p>Here’s just one interesting example: hooking into a live camera feed on
another person, to enable a person in a wheelchair to see herself
dancing on her feet:</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/vr-choreography.jpg" alt="VR choreography"></p>
<p class="caption">Credit: <a href="http://vimeo.com/74254297">BeAnotherLab</a></p>

<p>We’re only really just scratching the surface of it right now, but we
have the potential to create some amazing experiences for people, which
can lead to reactions like this:</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/vr-joy-grandma.jpg" alt="VR joy grandma"></p>
<p class="caption">Credit: <a href="http://youtu.be/pAC5SeNH8jw">Paul Rivot</a></p>

<p><strong>WebVR</strong></p>
<p>This is why I think Virtual Reality is exciting, and it’s an especially
exciting time for us Web developers right now. Because just in the last
few weeks:</p>
<ul>
<li>Apple finally <a href="http://blog.ludei.com/webgl-ios-8-safari-webview/">embraced
WebGL</a>, a key
technology for creating 3D experiences in the browser</li>
<li>And
<a href="http://blog.bitops.com/blog/2014/06/26/first-steps-for-vr-on-the-web/">Mozilla</a>
and
<a href="http://blog.tojicode.com/2014/07/bringing-vr-to-chrome.html">Google</a>
have both released special builds of their browsers, with initial
support for Virtual Reality</li>
</ul>
<p>This is what they’re implementing:</p>
<ul>
<li>The ability to discover available Virtual Reality devices (in
practice just the Oculus Rift right now, but more will
be coming…)</li>
<li>Full screen extensions so you can request an element goes full
screen on the VR headset</li>
<li>Sensor integration so you can use for example, the orientation
of the device</li>
<li>And the particular distortion effect required for rendering on
different VR devices - you should be able to be hardware
agnostic</li>
</ul>
<p>Google are calling this “WebVR” (Mozilla don’t seem to be naming it
anything in particular yet). It’s at “version zero” and it’s not even in
the alpha channels of the browsers yet; currently you can only get this
in separate builds.</p>
<p>Here’s how you use it… With a WebGL scene, you render it twice, side by
side: one for your left eye and one for your right eye.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/webgl-stereo.jpg" alt="VR stereo"></p>
<p class="caption">Credit: <a href="http://www.oculusvr.com/">Oculus Rift</a></p>

<p>The browser can apply the distortion required for the particular device</p>
<ul>
<li>it’s like this for the Oculus Rift:</li>
</ul>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/webgl-distortion.jpg" alt="VR distortion"></p>
<p class="caption">Credit: <a href="http://www.oculusvr.com/">Oculus Rift</a></p>

<p>This is what the lenses will turn into something that covers as much of
your vision as possible.</p>
<p>As for CSS3D content, it should be even easier because it’s declarative,
so you can leave it up to the browser to figure out how to render it.
You should just need to use ‘preserve-3d’ and set the ‘perspective’,
then just request that your containing element goes full-screen on the
VR device. That’s the theory anyway: Mozilla are working on this now,
but I haven’t seen any demos of it yet [Update: as of 31st July,
Mozilla have released <a href="http://blog.bitops.com/blog/2014/07/31/css-and-vr-integration/">new builds with preliminary CSS
integration</a>].</p>
<p>We’ll stick with WebGL and I’ll show you just the key pieces of code we
need to add. Warning: these APIs are brand new - they will undoubtedly
change.</p>
<pre><code>if( navigator.getVRDevices ) {
  navigator.getVRDevices().then( vrDeviceCallback );
}</code></pre><p>This is the discovery bit. [Update: Chrome and Firefox now both use
promises].</p>
<pre><code>function vrDeviceCallback( vrDevices ) {
    for( var i=0; i &lt; vrDevices.length; i++ ) {
        // If instance of HMDVRDevice...
        // If instance of PositionSensorVRDevice...
    }
}</code></pre><p>In our callback we can check it’s a Head-Mounted Display and also see if
we can get sensor data out for the orientation.</p>
<pre><code>var leftFOV =
    vrHMD.getRecommendedEyeFieldOfView(&#39;left&#39;);

var leftTrans = vrHMD.getEyeTranslation(&#39;left&#39;);</code></pre><p>For each eye, we can ask for the recommended field of view which we can
use to set the right camera projection, and also the translation to
apply, as in how far apart the cameras should be.</p>
<pre><code>if( canvas.webkitRequestFullscreen ) {
    canvas.webkitRequestFullscreen({
        vrDisplay: hmdDevice });

} else if( container.mozRequestFullScreen ) {
    container.mozRequestFullScreen({
        vrDisplay: hmdDevice });
}</code></pre><p>And we call requestFullscreen, passing in our VR device. (Note that for
Chrome “Fullscreen” has a small ‘s’ and you need to do it on the actual
actual WebGL canvas element. For Firefox, it’s a big ‘S’ and their
example calls it on the element containing the canvas).</p>
<p>Now you just need to add your usual WebGL goodness. I used the popular
<a href="http://threejs.org/">Three.js</a> library. I also like dinosaurs, so I
added a dinosaur thanks to <a href="http://dk.com/">Dorling Kindersley</a>. Plus a
sky map from <a href="http://www.eyeonline.com/">eyeon Software</a>. And I made
this…</p>
<p>I’m hoping to release the code, but I haven’t been able to yet. Brandon
Jones from Google has the <a href="http://bit.ly/oculus-example">code for his demo up
here</a> though, plus be sure to check out
<a href="http://blog.tojicode.com/2014/07/bringing-vr-to-chrome.html">his blog
post</a>.</p>
<p><strong>Google Cardboard</strong></p>
<p>Also just in the last few weeks, Google unveiled
<a href="https://developers.google.com/cardboard/">Cardboard</a> which, as it
sounds, is literally made out of cardboard, but it just takes a couple
of lenses and a button made out of a magnet, and it can turn your
existing smartphone into a rudimentary Virtual Reality device for just a
few dollars.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/google-cardboard.jpg" alt="Google Cardboard"></p>
<p>We can also create Cardboard apps using Web technologies, right now.
It’s not supported by these very new WebVR implementations just yet, but
because it’s essentially just a phone, we don’t actually need WebVR to
be able to get it to work.</p>
<p>In fact, Three.js has a StereoEffect we can apply, which makes it
easy to render the same scene for both eyes side by side:</p>
<pre><code>var effect = new THREE.StereoEffect( renderer );
...
effect.render( scene, camera );</code></pre><p>And Three.js also has a controls module that uses the standard HTML5
orientation API in order to render things according to the orientation
of the phone:</p>
<pre><code>var controls = new THREE.DeviceOrientationControls(
        camera, true);

controls.connect();
...
controls.update();</code></pre><p>Here’s what that looks like:</p>
<p>Again, unfortunately I can’t share the code for this right now, but
Google have a <a href="http://bit.ly/cardboard-example">code example up here</a>.</p>
<p><strong>Just a taster</strong></p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/trex.png" alt="TRex"></p>
<p>Hopefully I’ve given just a taster about some exciting new technology
coming up, and how you can get started with Virtual Reality right now.</p>
<p>I’ll leave you with this thought…</p>
<p>Today, we’re creating pictures under glass.</p>
<p>Tomorrow we’ll create whole new worlds.</p>
<p><img src="/images/posts/2014-08-04-interfaces-of-the-future/new-world.png" alt="New world"></p>
<p class="caption">Credit: <a href="http://mind-criminal.deviantart.com/art/Universe-From-a-Meadow-257284639">mind-criminal</a></p>

<p>So let’s get ahead of the curve and start hacking now!</p>
<p>Thank you.</p>

    <p class="tags">
      <span>talks</span>
      <span> future-tech</span>
      <span> virtual-reality</span>
      <span> augmented-reality</span>
    </p>
  </article>
</div>
    <footer>
      <p><a rel="me" href="https://toot.cafe/@peter">Mastodon</a> | <a rel="me" href="https://twitter.com/@poshaughnessy">Twitter</a> | <a href="https://github.com/poshaughnessy/">Github</a> | <a href="http://uk.linkedin.com/in/poshaughnessy/">LinkedIn</a> | <a href="https://github.com/poshaughnessy/peteroshaughnessy.com">Source</a> | <a href="rss.xml">RSS</a></p>
      <p>© 2019 Peter O'Shaughnessy</p>
    </footer>
  </body>
</html>
